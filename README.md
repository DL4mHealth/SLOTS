# SLOTS
Semi-Supervised End-to-End  Contrastive Learning for Time Series Classification

Authors: Huili Cai (huilicai0721@hhu.edu.cn), Xiang Zhang (xiang.alan.zhang@gmail.com), and Xiaofeng Liu (xfliu@hhu.edu.cn)

Overview

This repository contains two processed datasets (except for three other datasets that are too large) and the codes of SLOTS (along with baselines) for manuscript Semi-Supervised End-To-End Contrastive Learning For Time Series Classification. We propose an end-to-end model called SLOTS (Semi-supervised Learning fOr Time clasSification). We evaluate SLOTS by comparing it with ten state-of-the-art methods across five datasets.The following illustration provides an overview of the conventional two-stage framework and our end-to-end framework. 
![image](https://github.com/DL4mHealth/SLOTS/assets/47804803/33b8eb33-7691-473b-8884-29bcc63ae157)

Key idea of SLOTS

In this paper, we present a novel semi-supervised framework that achieves optimal performance with minimal labeled samples and can be seamlessly integrated into various architectures. Our approach systematically combines unsupervised contrastive loss, supervised contrastive loss, and classification loss to jointly update the model, maximizing the utilization of information from the data. We present the model pipeline of the proposed SLOTS in Figure 2.

